{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tensorflow-project/FineTuning/blob/main/Experiments/CosineSimilarity4emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow-project/FineTuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHvXPnQkeyzf",
        "outputId": "a7057e3e-ecda-48b7-9e3f-d182f336150a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'FineTuning' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import csv\n",
        "from google.oauth2.credentials import Credentials\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "### agree to mounting your Google Drive for saving the new weights later on\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "py_file_location = \"/content/FineTuning/four_emotions\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "py_file_location = \"/content/FineTuning/models\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "import textual_inversion_four_emotions as txt"
      ],
      "metadata": {
        "id": "SD_KkNtNSth_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b735bacd-64bf-4174-8a70-754ce9e3da41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### create an instance of the StableDiffusion() class\n",
        "stable_diffusion = txt.StableDiffusion()\n",
        "\n",
        "### our new concept which is later inserted in the different prompts (for training and image generation).\n",
        "###The goal is to create an embedding for our placeholder_token\n",
        "placeholder_token = \"\"\n",
        "\n",
        "\n",
        "### Add our placeholder_token to our stable_diffusion Model\n",
        "stable_diffusion.tokenizer.add_tokens(placeholder_token)\n",
        "\n",
        "train_ds = txt.create_dataset(stable_diffusion, placeholder_token)\n",
        "\n",
        "### beta is the diffusion rate \n",
        "noise_scheduler = txt.NoiseScheduler(\n",
        "    ### beta_start determines the amount of noise added at the start of the denoising process\n",
        "    beta_start=0.00085,\n",
        "    ### beta_end at the end of the denoising process\n",
        "    beta_end=0.012,\n",
        "    ### the beta_schedule determines that the diffusion rate increases linearly\n",
        "    beta_schedule=\"scaled_linear\",\n",
        "    train_timesteps=1000,\n",
        ")\n",
        "\n",
        "training_image_encoder = keras.Model(\n",
        "        stable_diffusion.image_encoder.input,\n",
        "        stable_diffusion.image_encoder.layers[-2].output,\n",
        "    )\n",
        "    \n",
        "    \n",
        "#EPOCHS = 50\n",
        "### learning rate decays depending on the number of epochs to avoid convergence issues in few epochs \n",
        "### in the originial tutorial a scheduler is used but we experienced to have better results without a scheduler\n",
        "\"\"\"learning_rate = keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=1e-4, decay_steps=train_ds.cardinality() * EPOCHS\n",
        ")\"\"\"\n",
        "### inizialize the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    weight_decay=0.004, learning_rate=1e-4, epsilon=1e-8, global_clipnorm=10\n",
        ")\n",
        "\n",
        "txt.textual_preprocessing(stable_diffusion, placeholder_token)\n",
        "\n",
        "### embeddings for cosine similarity\n",
        "sticker_embedding = []\n",
        "cosine_similarity = []\n",
        "cosine_similarity.append(txt.cosine_sim(txt.get_embedding(\"broccoli\", stable_diffusion), txt.get_embedding(placeholder_token, stable_diffusion)))"
      ],
      "metadata": {
        "id": "j6M4tCdlGhgm",
        "outputId": "6e9e3348-5e13-4a08-c628-c01cf3e03a14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/FineTuning/four_emotions/textual_inversion_four_emotions.py:536: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  sim = dot(e1, e2)/(norm(e1)*norm(e2))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### how long to train for\n",
        "epochs = 2\n",
        "epoch_num = list(range(epochs+1))\n",
        "txt.training(epochs, stable_diffusion, train_ds, sticker_embedding, cosine_similarity, stable_diffusion, noise_scheduler, training_image_encoder, optimizer, placeholder_token)"
      ],
      "metadata": {
        "id": "LD07AvQddezX",
        "outputId": "6459455b-19f7-48c7-cac8-63993e8dc21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:   0%|          | 0/535 [00:21<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "StagingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-831879c7966b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstable_diffusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msticker_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstable_diffusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_image_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/FineTuning/four_emotions/textual_inversion_four_emotions.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epoch, model, data, sticker_embedding, cosine_similarity, stable_diffusion, noise_scheduler, new_image_encoder, optimizer, placeholder_token)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {i+1}/{epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;31m# Compute the forward pass of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextual_inversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstable_diffusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_image_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_image_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m### Compute the embedding of the placeholder token and the cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    File \"/content/FineTuning/four_emotions/textual_inversion_four_emotions.py\", line 521, in textual_inversion  *\n        optimizer.apply_gradients(zip(gradients, trainable_weights))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 208, in _update_step_xla  *\n        return self._update_step(gradient, variable)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 224, in _update_step  **\n        self.update_step(gradient, variable)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_experimental/adam.py\", line 167, in update_step\n        m = self._momentums[self._index_dict[var_key]]\n\n    IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### where to store your newly generated weights\n",
        "path = '/content/drive/MyDrive/weights_four_emotions.npy'"
      ],
      "metadata": {
        "id": "o4OtGsg46sfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### get the weights of the text encoder and save the to Google Drive\n",
        "text_encoder_weights = np.array(stable_diffusion.text_encoder.get_weights(), dtype=object)\n",
        "\n",
        "### save the weights array to a file on your Google Drive\n",
        "np.save(path, text_encoder_weights)"
      ],
      "metadata": {
        "id": "1wBN4HdceChs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "dUk7L_UBkjBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()  # Create a new figure\n",
        "plt.plot(epoch_num, cosine_similarity, 'darkblue')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cosine Similarity')\n",
        "plt.title('Cosine Similarity')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Images/plot.jpg')"
      ],
      "metadata": {
        "id": "xK-Sa-nsOYJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}