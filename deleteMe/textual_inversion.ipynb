{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tensorflow-project/FineTuning/blob/main/textual_inversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/keras-team/keras-cv.git\n",
        "!pip install -q tensorflow==2.11.0\n",
        "!pip install pyyaml h5py\n",
        "\n",
        "### clone our Github Repository\n",
        "!git clone https://github.com/tensorflow-project/FineTuning\n",
        "%cd FineTuning/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBGcW8Cv9eyo",
        "outputId": "b48b4f94-3a20-46de-b3ab-99a1e4b6f14e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-cv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.9/dist-packages (from h5py) (1.22.4)\n",
            "Cloning into 'FineTuning'...\n",
            "remote: Enumerating objects: 344, done.\u001b[K\n",
            "remote: Counting objects: 100% (262/262), done.\u001b[K\n",
            "remote: Compressing objects: 100% (169/169), done.\u001b[K\n",
            "remote: Total 344 (delta 194), reused 124 (delta 91), pack-reused 82\u001b[K\n",
            "Receiving objects: 100% (344/344), 72.25 MiB | 15.63 MiB/s, done.\n",
            "Resolving deltas: 100% (253/253), done.\n",
            "/content/FineTuning/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras_cv import layers as cv_layers\n",
        "from keras_cv.models.stable_diffusion import NoiseScheduler\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import os\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "\n",
        "### import the different models from our Github repository\n",
        "from text_encoder import TextEncoder\n",
        "from decoder import Decoder\n",
        "from diffusion_model import DiffusionModel\n",
        "from stable_diffusion import StableDiffusion\n",
        "\n",
        "\n",
        "### create an instance of the StableDiffusion() class\n",
        "stable_diffusion = StableDiffusion()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbd_Eat9rig",
        "outputId": "664cb069-1cfd-47df-c3dd-6855590ff4e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(images):\n",
        "    \"\"\"function to plot images in subplots\n",
        "     Args: \n",
        "      - images: numpy arrays we want to visualize\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(images)):\n",
        "        ax = plt.subplot(1, len(images), i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "QIjiaD179vF3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assemble_image_dataset(urls):\n",
        "    \"\"\"Downloads a list of image URLs, resizes and normalizes the images, shuffles them, and adds random noise to create a \n",
        "    TensorFlow dataset object for them. \n",
        "\n",
        "    Args:\n",
        "    - urls: A list of image URLs to download and use for the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - image_dataset: A TensorFlow dataset object containing the preprocessed images.\n",
        "\n",
        "    Notes:\n",
        "    - This function assumes that all images have the same dimensions and color channels. \n",
        "    \"\"\"\n",
        "  \n",
        "    # Fetch all remote files\n",
        "    files = [tf.keras.utils.get_file(origin=url) for url in urls]\n",
        "\n",
        "    # Resize images\n",
        "    resize = keras.layers.Resizing(height=512, width=512, crop_to_aspect_ratio=True)\n",
        "    images = [keras.utils.load_img(img) for img in files]\n",
        "    images = [keras.utils.img_to_array(img) for img in images]\n",
        "    images = np.array([resize(img) for img in images])\n",
        "\n",
        "    # The StableDiffusion image encoder requires images to be normalized to the\n",
        "    # [-1, 1] pixel value range\n",
        "    images = images / 127.5 - 1\n",
        "\n",
        "    # Create the tf.data.Dataset\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "\n",
        "    # Shuffle and introduce random noise\n",
        "    image_dataset = image_dataset.shuffle(50, reshuffle_each_iteration=True)\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomCropAndResize(\n",
        "            target_size=(512, 512),\n",
        "            crop_area_factor=(0.8, 1.0),\n",
        "            aspect_ratio_factor=(1.0, 1.0),\n",
        "        ),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomFlip(mode=\"horizontal\"),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    return image_dataset"
      ],
      "metadata": {
        "id": "uGQVN9sefI3n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_PROMPT_LENGTH = 77\n",
        "\n",
        "### our new concept which is later inserted in the different prompts (for training and image generation).\n",
        "###The goal is to create an embedding for our placeholder_token\n",
        "placeholder_token = \"<my-broccoli-token>\"\n",
        "\n",
        "\n",
        "def pad_embedding(embedding):\n",
        "    \"\"\"Pads the input embedding with the end-of-text token to ensure that it has the same length as the maximum prompt length.\n",
        "\n",
        "    Args:\n",
        "    - embedding: A list of tokens representing the input embedding.\n",
        "\n",
        "    Returns:\n",
        "    - padded_embedding: A list of tokens representing the padded input embedding.\n",
        "    \"\"\"\n",
        "    return embedding + (\n",
        "        [stable_diffusion.tokenizer.end_of_text] * (MAX_PROMPT_LENGTH - len(embedding))\n",
        "    )\n",
        "\n",
        "### Add our placeholder_token to our stable_diffusion Model\n",
        "stable_diffusion.tokenizer.add_tokens(placeholder_token)\n",
        "\n",
        "\n",
        "def assemble_text_dataset(prompts, placeholder_token):\n",
        "    \"\"\"Creates a text dataset consisting of prompt embeddings. \n",
        "\n",
        "    Args:\n",
        "    - prompts: A list of string prompts to be encoded and turned into embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - text_dataset: A text dataset containing the prompt embeddings.\n",
        "    \"\"\"\n",
        "    ### inserts our placeholder_token into the different prompts\n",
        "    prompts = [prompt.format(placeholder_token) for prompt in prompts]\n",
        "\n",
        "    ### prompts are tokenized and encoded and then embeddings are padded\n",
        "    embeddings = [stable_diffusion.tokenizer.encode(prompt) for prompt in prompts]\n",
        "    embeddings = [np.array(pad_embedding(embedding)) for embedding in embeddings]\n",
        "\n",
        "    ### creates a dataset consisting of the different prompt embeddings and shuffles it\n",
        "    text_dataset = tf.data.Dataset.from_tensor_slices(embeddings)\n",
        "    text_dataset = text_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
        "    return text_dataset"
      ],
      "metadata": {
        "id": "qzfpFurp-JWE",
        "outputId": "89fb3e03-8748-44fc-83e3-0d132e03bde5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n",
            "1356917/1356917 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assemble_dataset(urls, prompts, placeholder_token):\n",
        "    \"\"\" Assembles a TensorFlow Dataset containing pairs of images and text prompts.\n",
        "\n",
        "    Args:\n",
        "    - urls: A list of URLs representing the image dataset\n",
        "    - prompts: A list of text prompts corresponding to the images\n",
        "    - placeholder_token: A string token representing the location where the prompt text will be inserted in the final text\n",
        "\n",
        "    Returns:\n",
        "    - A TensorFlow Dataset object containing pairs of images and their corresponding text prompts.\n",
        "    \"\"\"\n",
        "    ### creating the image and test dataset\n",
        "    image_dataset = assemble_image_dataset(urls)\n",
        "    text_dataset = assemble_text_dataset(prompts, placeholder_token)\n",
        "    \n",
        "    ### repeat both datasets to get several different combinations of images and text prompts\n",
        "    # the image dataset is quite short, so we repeat it to match the length of the text prompt dataset\n",
        "    image_dataset = image_dataset.repeat()\n",
        "\n",
        "    # we use the text prompt dataset to determine the length of the dataset.  Due to\n",
        "    # the fact that there are relatively few prompts we repeat the dataset 5 times.\n",
        "    # we have found that this anecdotally improves results.\n",
        "    text_dataset = text_dataset.repeat(5)\n",
        "    return tf.data.Dataset.zip((image_dataset, text_dataset))"
      ],
      "metadata": {
        "id": "jb4j44pw-NvF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### create a dataset consisting of happy broccoli stickers and happy prompts\n",
        "happy_ds = assemble_dataset(\n",
        "    urls = [\n",
        "        \"https://i.imgur.com/9zAwPyt.jpg\",\n",
        "        \"https://i.imgur.com/qCNFRl4.jpg\",\n",
        "        \"https://i.imgur.com/Q8qiVEN.jpg\",\n",
        "    ],\n",
        "    prompts = [\n",
        "        \"a photo of a happy {}\",\n",
        "        \"a photo of {}\",\n",
        "        \"a photo of one {}\",\n",
        "        \"a photo of a nice {}\",\n",
        "        \"a good photo of a {}\",\n",
        "        \"a photo of the nice {}\",\n",
        "        \"a photo of a cool {}\",\n",
        "        \"a rendition of the {}\",\n",
        "        \"a nice sticker of a {}\",\n",
        "        \"a sticker of a {}\",\n",
        "        \"a sticker of a happy {}\",\n",
        "        \"a sticker of a lucky {}\",\n",
        "        \"a sticker of a lovely {}\",\n",
        "        \"a sticker of a {} in a positive mood\",\n",
        "        \"a pixar chracter of a satisfied {}\",\n",
        "        \"a disney character of a positive {}\",\n",
        "        \"a sticker of a delighted {}\",\n",
        "        \"a sticker of a joyful {}\",\n",
        "        \"a sticker of a cheerful {}\",\n",
        "        \"a drawing of a glad {}\",\n",
        "        \"a sticker of a merry {}\",\n",
        "        \"a sticker of a pleased {}\",\n",
        "    ],\n",
        "    placeholder_token = placeholder_token\n",
        ")"
      ],
      "metadata": {
        "id": "f34i5Qk_-30E",
        "outputId": "10d908f9-a87b-4bfd-cce2-9bc5653550cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://i.imgur.com/9zAwPyt.jpg\n",
            "8784/8784 [==============================] - 0s 1us/step\n",
            "Downloading data from https://i.imgur.com/qCNFRl4.jpg\n",
            "14548/14548 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/Q8qiVEN.jpg\n",
            "8810/8810 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "love_ds = assemble_dataset(\n",
        "    urls = [\n",
        "        \"https://i.imgur.com/SqFxJfM.jpg\",\n",
        "        \"https://i.imgur.com/hFqqp3p.jpg\",\n",
        "        \"https://i.imgur.com/uGkSrzg.jpg\",\n",
        "    ],\n",
        "    prompts = [\n",
        "        \"a photo of a {} in love\",\n",
        "        \"a photo of {}\",\n",
        "        \"a photo of one {}\",\n",
        "        \"a photo of a nice {}\",\n",
        "        \"a good photo of a {}\",\n",
        "        \"a photo of the nice {}\",\n",
        "        \"a photo of an amorous {}\",\n",
        "        \"a rendition of the {}\",\n",
        "        \"a nice sticker of a {}\",\n",
        "        \"a sticker of a {}\",\n",
        "        \"a sticker of a {} in love\",\n",
        "        \"a sticker of an amorous {}\",\n",
        "        \"a sticker of a lovely {}\",\n",
        "        \"a sticker of a {} in a positive mood\",\n",
        "        \"a pixar chracter of a {} in love\",\n",
        "        \"a disney character of a positive {}\",\n",
        "        \"a sticker of a delighted {}\",\n",
        "        \"a sticker of a joyful {}\",\n",
        "        \"a drawing of a {} in love\",\n",
        "        \"a drawing of a glad {}\",\n",
        "        \"a sticker of a loving {}\",\n",
        "        \"a sticker of a pleased {}\",\n",
        "    ],\n",
        "    placeholder_token = placeholder_token\n",
        ")"
      ],
      "metadata": {
        "id": "yAEyvTAnniUA",
        "outputId": "916be726-87af-4b59-e074-f1165ab855ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://i.imgur.com/SqFxJfM.jpg\n",
            "15475/15475 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/hFqqp3p.jpg\n",
            "7870/7870 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/uGkSrzg.jpg\n",
            "10306/10306 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sad_ds = assemble_dataset(\n",
        "    urls = [\n",
        "        \"https://i.imgur.com/hlkuxBX.jpg\",\n",
        "        \"https://i.imgur.com/kPH9XIh.jpg\",\n",
        "        \"https://i.imgur.com/OR2oxyK.jpg\",\n",
        "    ],\n",
        "    prompts = [\n",
        "        \"a photo of a sad {}\",\n",
        "        \"a photo of {}\",\n",
        "        \"a photo of one {}\",\n",
        "        \"a photo of an unhappy {}\",\n",
        "        \"a good photo of a {}\",\n",
        "        \"a photo of the unhappy {}\",\n",
        "        \"a photo of a depressed {}\",\n",
        "        \"a rendition of the sad {}\",\n",
        "        \"a nice sticker of a miserable {}\",\n",
        "        \"a sticker of a {}\",\n",
        "        \"a sticker of a downhearted {}\",\n",
        "        \"a sticker of a sorrowful {}\",\n",
        "        \"a sticker of an unhappy {}\",\n",
        "        \"a sticker of a {} in a negative mood\",\n",
        "        \"a pixar chracter of a depressed {}\",\n",
        "        \"a disney character of a negative {}\",\n",
        "        \"a sticker of a mourning {}\",\n",
        "        \"a sticker of a grieving {}\",\n",
        "        \"a drawing of a sad {}\",\n",
        "        \"a drawing of a miserable {}\",\n",
        "        \"a sticker of a sorrowful {}\",\n",
        "        \"a sticker of a sobbing {}\",\n",
        "    ],\n",
        "    placeholder_token=placeholder_token\n",
        ")"
      ],
      "metadata": {
        "id": "lbZX8CnnnmQZ",
        "outputId": "c5b270d1-1a39-4973-d2f3-3569b118964d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://i.imgur.com/hlkuxBX.jpg\n",
            "11991/11991 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/kPH9XIh.jpg\n",
            "14897/14897 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/OR2oxyK.jpg\n",
            "8773/8773 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "angry_ds = assemble_dataset(\n",
        "    urls = [\n",
        "        \"https://i.imgur.com/mZswnIx.jpg\",\n",
        "        \"https://i.imgur.com/TmlHZRY.png\",\n",
        "        \"https://i.imgur.com/BmVIZlO.png\",\n",
        "    ],\n",
        "    prompts = [\n",
        "        \"A photo of an angry {}\",        \n",
        "        \"A photo of a {} with a furious expression\",        \n",
        "        \"A photo of a {} in a fit of rage\",        \n",
        "        \"A photo of a {} yelling in anger\",        \n",
        "        \"A photo of a {} with a scowling face\",        \n",
        "        \"A photo of a {} clenching their fists in anger\",        \n",
        "        \"A photo of a {} looking aggressive\",        \n",
        "        \"A photo of a {} with an angry glare\",        \n",
        "        \"A photo of a {} shouting in fury\",        \n",
        "        \"A photo of a {} in a heated argument\",        \n",
        "        \"A photo of a {} with a fiery temper\",        \n",
        "        \"A photo of a {} with a hostile expression\",        \n",
        "        \"A photo of a {} with a look of outrage\",        \n",
        "        \"A photo of a {} with a seething rage\",        \n",
        "        \"A photo of a {} with a temper tantrum\",        \n",
        "        \"A photo of a {} with a wrathful expression\",        \n",
        "        \"A photo of a {} fuming with anger\",        \n",
        "        \"A photo of a {} with a burning rage\",        \n",
        "        \"A photo of a {} with a stormy expression\",        \n",
        "        \"A photo of a {} with a resentful look\",        \n",
        "        \"A photo of a {} with an irate expression\", \n",
        "        \"a sticker of a {}\",\n",
        "        \"a sticker of an angry {}\",\n",
        "        \"a sticker of a mad {}\",\n",
        "        \"a sticker of an annoyed {}\",\n",
        "        \"a sticker of a {} in a negative mood\",\n",
        "        \"a pixar chracter of an enraged {}\",\n",
        "        \"a disney character of a negative {}\",\n",
        "        \"a sticker of a furious {}\",\n",
        "        \"a sticker of an upset {}\",\n",
        "        \"a drawing of an angry {}\",\n",
        "        \"a drawing of a miserable {}\",\n",
        "        \"a sticker of a miserable {}\",\n",
        "        \"a sticker of an enraged {}\",\n",
        "    ],\n",
        "    placeholder_token = placeholder_token,\n",
        ")"
      ],
      "metadata": {
        "id": "L9Wy8_XDnpLk",
        "outputId": "25fb9217-5ace-4a3f-d7bc-6f7c8fb008e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://i.imgur.com/mZswnIx.jpg\n",
            "6822/6822 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/TmlHZRY.png\n",
            "141537/141537 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/BmVIZlO.png\n",
            "201997/201997 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### concatenate the different datasets with the different emotions\n",
        "positive_ds = happy_ds.concatenate(love_ds)\n",
        "negative_ds = sad_ds.concatenate(angry_ds)\n",
        "train_ds = positive_ds.concatenate(negative_ds)\n",
        "train_ds = train_ds.batch(1).shuffle(\n",
        "    train_ds.cardinality(), reshuffle_each_iteration=True\n",
        ")"
      ],
      "metadata": {
        "id": "Tcyypfez_c0d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### defining concept we want to build our new concept on\n",
        "tokenized_initializer = stable_diffusion.tokenizer.encode(\"broccoli\")[1]\n",
        "\n",
        "### get the embedding of our basis concept to clone it to our new placeholder's embedding\n",
        "new_weights = stable_diffusion.text_encoder.layers[2].token_embedding(tf.constant(tokenized_initializer))\n",
        "\n",
        "# Get len of .vocab instead of tokenizer\n",
        "new_vocab_size = len(stable_diffusion.tokenizer.vocab)\n",
        "\n",
        "# The embedding layer is the 2nd layer in the text encoder\n",
        "### get the weights of the concept we build on ('broccoli')\n",
        "old_token_weights = stable_diffusion.text_encoder.layers[2].token_embedding.get_weights()\n",
        "old_position_weights = stable_diffusion.text_encoder.layers[2].position_embedding.get_weights()\n",
        "\n",
        "### unpack the old weights\n",
        "old_token_weights = old_token_weights[0]\n",
        "\n",
        "### old_token_weights has now the shape (vocab_size, embedding_dim)\n",
        "### expand the dimension to be able to concatenate it with old_token_weights\n",
        "new_weights = np.expand_dims(new_weights, axis=0)\n",
        "new_weights = np.concatenate([old_token_weights, new_weights], axis=0)"
      ],
      "metadata": {
        "id": "9PqYW3Qz_7wz",
        "outputId": "4164194f-6ae7-46a8-dee1-71d9d4d744d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\n",
            "492466864/492466864 [==============================] - 5s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Have to set download_weights False so we can initialize the weigths ourselves\n",
        "### create a new text encoder \n",
        "new_encoder = TextEncoder(\n",
        "    MAX_PROMPT_LENGTH,\n",
        "    vocab_size = new_vocab_size,\n",
        "    download_weights = False,\n",
        ")\n",
        "### we set the weights of the new_encoder to the same as in the old text_encoder except from the embedding layer\n",
        "for index, layer in enumerate(stable_diffusion.text_encoder.layers):\n",
        "    # Layer 2 is the embedding layer, so we omit it from our weight-copying\n",
        "    if index == 2:\n",
        "        continue\n",
        "    new_encoder.layers[index].set_weights(layer.get_weights())\n",
        "\n",
        "### set the weights of the embedding layer according to our new_weights\n",
        "new_encoder.layers[2].token_embedding.set_weights([new_weights])\n",
        "\n",
        "### set all weights of the other embeddings to the same values as in the initial text encoder\n",
        "new_encoder.layers[2].position_embedding.set_weights(old_position_weights)\n",
        "\n",
        "### set the stable_diffusion text encoder to our new_encoder and compile it\n",
        "### thus the stable_diffusion.text_encoder has the adjusted weights\n",
        "stable_diffusion._text_encoder = new_encoder\n",
        "stable_diffusion._text_encoder.compile(jit_compile=True)"
      ],
      "metadata": {
        "id": "Yad0-aJh__Af"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### we only train the encoder as we want to fine-tune the embeddings\n",
        "stable_diffusion.diffusion_model.trainable = False\n",
        "stable_diffusion.decoder.trainable = False\n",
        "stable_diffusion.text_encoder.trainable = True\n",
        "\n",
        "stable_diffusion.text_encoder.layers[2].trainable = True\n",
        "\n",
        "def traverse_layers(layer):\n",
        "    \"\"\" Traverses the layers and embedding attributes of a layer\n",
        "\n",
        "    Args:\n",
        "    - layer: A text encoder layer\n",
        "\n",
        "    Yields:\n",
        "    -  layers and their corresponding embedding attributes\n",
        "    \"\"\"\n",
        "    if hasattr(layer, \"layers\"):\n",
        "        for layer in layer.layers:\n",
        "            yield layer\n",
        "    if hasattr(layer, \"token_embedding\"):\n",
        "        yield layer.token_embedding\n",
        "    if hasattr(layer, \"position_embedding\"):\n",
        "        yield layer.position_embedding\n",
        "\n",
        "### iterates through the generator and adjusts the trainable attribute of the layers to trainable = True if it is part of the embedding\n",
        "for layer in traverse_layers(stable_diffusion.text_encoder):\n",
        "    if isinstance(layer, keras.layers.Embedding) or \"clip_embedding\" in layer.name:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "### set the layer that only encodes the position of tokens in the prompts to trainable = False\n",
        "new_encoder.layers[2].position_embedding.trainable = False"
      ],
      "metadata": {
        "id": "1OejLLnMADTd",
        "outputId": "61c15d50-9b5a-4b9e-90cb-48b4cafc3733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5\n",
            "3439090152/3439090152 [==============================] - 40s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5\n",
            "198180272/198180272 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### put all the different components of stable diffusion model into a list\n",
        "all_models = [\n",
        "    stable_diffusion.text_encoder,\n",
        "    stable_diffusion.diffusion_model,\n",
        "    stable_diffusion.decoder,\n",
        "]\n",
        "\n",
        "### check that only in the text encoder we have trainable weights\n",
        "print([[w.shape for w in model.trainable_weights] for model in all_models])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svfYf616AMqX",
        "outputId": "cc3f41d0-dff2-485a-aa1d-430f2de07cec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[TensorShape([49409, 768])], [], []]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the top layer from the encoder, which cuts off the variance and only returns the mean\n",
        "### we make the encoder more efficient while still preserving the most important features\n",
        "training_image_encoder = keras.Model(\n",
        "    stable_diffusion.image_encoder.input,\n",
        "    stable_diffusion.image_encoder.layers[-2].output,\n",
        ")\n",
        "\n",
        "\n",
        "def sample_from_encoder_outputs(outputs):\n",
        "    \"\"\"Returns a random sample from the embedding distribution given the mean and log variance tensors\n",
        "\n",
        "    Args:\n",
        "    - outputs: A tensor of shape (batch_size, embedding_dim*2), where the first embedding_dim values correspond to the mean of the distribution, \n",
        "               and the second embedding_dim values correspond to the log variance of the distribution\n",
        "\n",
        "    Returns:\n",
        "    - a tensor of shape (batch_size, embedding_dim), representing a random sample from the embedding distribution\n",
        "    \"\"\"\n",
        "    mean, logvar = tf.split(outputs, 2, axis=-1)\n",
        "    logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n",
        "    std = tf.exp(0.5 * logvar)\n",
        "    sample = tf.random.normal(tf.shape(mean))\n",
        "    return mean + std * sample\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timestep, dim=320, max_period=10000):\n",
        "    \"\"\"Returns the embedding of a specific timestep in the denoising process\n",
        "\n",
        "    Args:\n",
        "    - timestep (int): The timestep for which the embedding is requested\n",
        "    - dim (int, optional): The dimensionality of the embedding, default is 320\n",
        "    - max_period (int, optional): The maximum period, default is 10000\n",
        "\n",
        "    Returns:\n",
        "    - embedding (tf.Tensor): A tensor of shape (dim,) containing the embedding of the specified timestep\n",
        "    \"\"\"\n",
        "    ### calculate half the dimensionality of the embedding\n",
        "    half = dim // 2\n",
        "    \n",
        "    ### calculate frequencies using logarithmically decreasing values\n",
        "    freqs = tf.math.exp(\n",
        "        -math.log(max_period) * tf.range(0, half, dtype=tf.float32) / half\n",
        "    )\n",
        "    \n",
        "    ### compute arguments for cosine and sine functions\n",
        "    args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
        "    \n",
        "    ### concatenate cosine and sine values to create embedding\n",
        "    embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
        "    \n",
        "    ### return the embedding tensor\n",
        "    return embedding\n",
        "\n",
        "#### used for hidden state (output of text encoder)\n",
        "def get_position_ids():\n",
        "    \"\"\"returns position IDs for the transformer model,\n",
        "        the IDs range from 0 to MAX_PROMPT_LENGTH-1\n",
        "\n",
        "    Returns:\n",
        "    - position_ids (tf.Tensor): A tensor of shape (1, MAX_PROMPT_LENGTH) containing the position IDs\n",
        "    \"\"\"\n",
        "    \n",
        "    ### create a list of integers from 0 to MAX_PROMPT_LENGTH-1\n",
        "    positions = list(range(MAX_PROMPT_LENGTH))\n",
        "    \n",
        "    ### convert the list to a tensor with dtype int32\n",
        "    position_ids = tf.convert_to_tensor([positions], dtype=tf.int32)\n",
        "    \n",
        "    return position_ids\n"
      ],
      "metadata": {
        "id": "OcyPGc6hAQQc",
        "outputId": "9d899044-6c1d-4977-80ec-e3f22afaa0f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/vae_encoder.h5\n",
            "136824240/136824240 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def textual_inversion(model, noise_scheduler, data):\n",
        "    \"\"\"Performs textual inversion using a given model and noise scheduler. Uses a gradient tape to calculate the mean squared error between predicted noise and actual noise,\n",
        "     uses this loss to update the weights of the text encoder with the goal of only training the embedding of the placeholder token\n",
        "\n",
        "    Arguments:\n",
        "    - model: A model that takes in noisy latents, timestep embeddings, and the output of the text encoder, and predicts noise\n",
        "    - noise_scheduler: A noise scheduler that adds noise to latents based on a given timestep\n",
        "    - data: A tuple containing images and prompt embeddings\n",
        "\n",
        "    Returns:\n",
        "    - a dictionary containing the loss value of the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    images, prompt_embeddings = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        ### creating embeddings out of the images \n",
        "        image_embeddings = training_image_encoder(images)\n",
        "        ### pass the embeddings to the sampler and save some sammples in latents\n",
        "        latents = sample_from_encoder_outputs(image_embeddings)\n",
        "        ### match the latents with those used in the training of Stable Diffusion (just a random number they used in the training)\n",
        "        latents = latents * 0.18215\n",
        "\n",
        "        ### random noise in the same shape as latents\n",
        "        noise = tf.random.normal(tf.shape(latents))\n",
        "        \n",
        "        ### get the batch dimension of our input data\n",
        "        batch_dim = tf.shape(latents)[0]\n",
        "\n",
        "        ### for each sample in the batch we choose a different random timestep to later determine the specific timestep embedding\n",
        "        timesteps = tf.random.uniform((batch_dim,), minval=0, maxval=noise_scheduler.train_timesteps, dtype=tf.int64,)\n",
        "\n",
        "\n",
        "        ### add the noise corresponding to the timestep to the latents by use of the scheduler\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "        \n",
        "        ### tensor containing all possible indices\n",
        "        indices = get_position_ids()\n",
        "        \n",
        "        ### calculate the output of the encoder\n",
        "        output_encoder = model.text_encoder([prompt_embeddings, indices])\n",
        "        \n",
        "        ### getting the timestep embeddings for each timestep\n",
        "        timestep_embeddings = tf.map_fn(fn=get_timestep_embedding, elems=timesteps, fn_output_signature=tf.float32,)\n",
        "\n",
        "        ### calculate the noise predictions with help of the latents, the time step embeddings and the output of the encoder\n",
        "        noise_pred = model.diffusion_model([noisy_latents, timestep_embeddings, output_encoder])\n",
        "\n",
        "        ### compute the mean squared error between the noise and the predicted noise and reduce it by taking the mean\n",
        "        loss = tf.keras.losses.mean_squared_error(noise_pred, noise)\n",
        "        loss = tf.reduce_mean(loss, axis=2)\n",
        "        loss = tf.reduce_mean(loss, axis=1)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        ### load the the weights we want to train from the text encoder and calculate the gradients for them\n",
        "        trainable_weights = model.text_encoder.trainable_weights\n",
        "        gradients = tape.gradient(loss, trainable_weights)\n",
        "\n",
        "        ### we only want to update the gradient of the placeholder token, therefore we create the tensor condition which has the value true for the index of the placeholder token (49408) and otherwise false\n",
        "        condition = gradients[0].indices == 49408\n",
        "\n",
        "        ### add an extra dimension to later zero out the gradients for other tokens\n",
        "        condition = tf.expand_dims(condition, axis=-1)\n",
        "\n",
        "        ### freeze the weights for all tokens by setting the gradients to 0 except for the placeholder token\n",
        "        gradients[0] = tf.IndexedSlices(values=tf.where(condition, gradients[0].values,0), indices=gradients[0].indices, dense_shape=gradients[0].dense_shape)\n",
        "\n",
        "        ### apply the gradients to the trainable weights of the encoder and thus only training the placeholder token's embedding\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_weights))\n",
        "\n",
        "        return {\"loss\": loss}"
      ],
      "metadata": {
        "id": "v_Wk-M19QuPp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### beta is the diffusion rate \n",
        "noise_scheduler = NoiseScheduler(\n",
        "    ### beta_start determines the amount of noise added at the start of the denoising process\n",
        "    beta_start=0.00085,\n",
        "    ### beta_end at the end of the denoising process\n",
        "    beta_end=0.012,\n",
        "    ### the beta_schedule determines that the diffusion rate increases linearly\n",
        "    beta_schedule=\"scaled_linear\",\n",
        "    train_timesteps=1000,\n",
        ")\n",
        "\n",
        "\n",
        "#EPOCHS = 50\n",
        "### learning rate decays depending on the number of epochs to avoid convergence issues in few epochs \n",
        "### in the originial tutorial a scheduler is used but we experienced to have better results without a scheduler\n",
        "\"\"\"learning_rate = keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=1e-4, decay_steps=train_ds.cardinality() * EPOCHS\n",
        ")\"\"\"\n",
        "### inizialize the optimizer\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    weight_decay=0.004, learning_rate=1e-4, epsilon=1e-8, global_clipnorm=10\n",
        ")"
      ],
      "metadata": {
        "id": "9_-FN_UkAgQK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_sim(e1, e2):\n",
        "  sim = dot(e1, e2)/(norm(e1)*norm(e2))\n",
        "  return sim"
      ],
      "metadata": {
        "id": "wcW3uZmPRjb8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(token):\n",
        "  tokenized = stable_diffusion.tokenizer.encode(token)[1]\n",
        "  embedding = stable_diffusion.text_encoder.layers[2].token_embedding(tf.constant(tokenized))\n",
        "\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "qBT6md2VR6HD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sticker_embedding = []\n",
        "cosine_similarity = []\n",
        "broccoli = get_embedding(\"broccoli\")\n",
        "cosine_similarity.append(cosine_sim(broccoli, get_embedding(placeholder_token)))"
      ],
      "metadata": {
        "id": "7X5Igd_hSLO6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(epoch=5, model=stable_diffusion, data = train_ds):\n",
        "    for i in range(epoch):\n",
        "        for batch in data:\n",
        "            textual_inversion(model=stable_diffusion, noise_scheduler=noise_scheduler, data=batch)\n",
        "            \n",
        "        emb = get_embedding(placeholder_token)\n",
        "        sticker_embedding.append(emb)\n",
        "        cosine_similarity.append(cosine_sim(broccoli, emb))\n",
        "\n",
        "        generated = stable_diffusion.text_to_image(\n",
        "        f\"a happy {placeholder_token} \",\n",
        "        batch_size=3, \n",
        "        num_steps = 25, \n",
        "        seed=2746\n",
        "        )\n",
        "\n",
        "        plot_images(generated)"
      ],
      "metadata": {
        "id": "XhFxyX26SPBx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "epoch_num = list(range(epochs+1))\n",
        "training(epoch=epochs, model=stable_diffusion, data= train_ds)"
      ],
      "metadata": {
        "id": "DQdjV7NMSRMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_num, cosine_similarity)\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Cosine Similarity\")\n",
        "plt.title(\"Cosine Similarity between the basis and the new concept\")\n",
        "plt.show"
      ],
      "metadata": {
        "id": "_5l1WOIeSmpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "path = '/content/drive/MyDrive/angryweeeights.npy'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATajLqR_fDnD",
        "outputId": "507a8e21-517b-4bce-f19e-7d0d6b4c4ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###get the weights of the text encoder and save the to google drive\n",
        "text_encoder_weights = np.array(stable_diffusion.text_encoder.get_weights())\n",
        "\n",
        "### Save the weights array to a file on your Google Drive\n",
        "np.save(path, text_encoder_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnUZy9Jul-R3",
        "outputId": "5ddbe587-9e90-4b65-e356-b8f96a08ea84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-3985f7e94cf5>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  text_encoder_weights = np.array(stable_diffusion.text_encoder.get_weights())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###load the array of the weights of the text encoder from the last training from google drive\n",
        "text_encoder_weights = np.load(path, allow_pickle=True)\n",
        "\n",
        "### Set the weights of the text encoder\n",
        "stable_diffusion.text_encoder.set_weights(text_encoder_weights)"
      ],
      "metadata": {
        "id": "1tLrIWEfXeP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "drive_folder = '/content/drive/MyDrive/angrybildeeer/'\n",
        "\n",
        "### get the number, we stopped the last time to name our pictures, to ensure each picture gets a different name\n",
        "i_file = os.path.join(drive_folder, 'i.txt')\n",
        "if os.path.isfile(i_file):\n",
        "    with open(i_file, 'r') as f:\n",
        "        i = int(f.read())\n",
        "else:\n",
        "    i = 0\n",
        "\n",
        "for j in range(10):\n",
        "    generated = stable_diffusion.text_to_image(\n",
        "    f\" a angry {placeholder_token}.\", batch_size=1,  num_steps=25 )\n",
        "    broc = generated[0]\n",
        "\n",
        "    ### convert the array generated from our stable diffusion model into a picture\n",
        "    broc = Image.fromarray(broc, mode='RGB')\n",
        "\n",
        "    broc.save(f'image_{i}.jpg')\n",
        "\n",
        "    ### save the picture to google drive\n",
        "    local_path = f'image_{i}.jpg'\n",
        "    drive_path = os.path.join(drive_folder, f'image_{i}.jpg')  # Use f-string to include variable in file name\n",
        "    shutil.copy(local_path, drive_path)\n",
        "\n",
        "    ### store the value of i in the file, to ensure no picture will have the same name\n",
        "    i += 1\n",
        "    with open(i_file, 'w') as f:\n",
        "        f.write(str(i))"
      ],
      "metadata": {
        "id": "IepAZNtu2JfE",
        "outputId": "c230fc36-3655-43d7-c5e4-95f3e0307e91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 27s 1s/step\n",
            "25/25 [==============================] - 28s 1s/step\n",
            "25/25 [==============================] - 27s 1s/step\n",
            "25/25 [==============================] - 27s 1s/step\n",
            "25/25 [==============================] - 27s 1s/step\n",
            "25/25 [==============================] - 27s 1s/step\n",
            "25/25 [==============================] - 28s 1s/step\n",
            "25/25 [==============================] - 27s 1s/step\n",
            "25/25 [==============================] - 27s 1s/step\n",
            "25/25 [==============================] - 27s 1s/step\n"
          ]
        }
      ]
    }
  ]
}
