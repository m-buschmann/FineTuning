{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tensorflow-project/stable-diffusion/blob/main/keras_cv_text_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQRoVI0mzS5h"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 The KerasCV Authors\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.experimental import numpy as tfnp\n",
        "\n",
        "\n",
        "class TextEncoder(keras.Model):\n",
        "    def __init__(\n",
        "        self, max_length, vocab_size=49408, name=None, download_weights=True\n",
        "    ):  \n",
        "        tokens = keras.layers.Input(\n",
        "            shape=(max_length,), dtype=\"int32\", name=\"tokens\"\n",
        "        )\n",
        "        positions = keras.layers.Input(\n",
        "            shape=(max_length,), dtype=\"int32\", name=\"positions\"\n",
        "        )\n",
        "        ### construct an embedding\n",
        "        x = CLIPEmbedding(vocab_size, 768, max_length)([tokens, positions])\n",
        "\n",
        "        ### build 12 layers with the embedding dimension 768 and 12 attention heads with quick_gelu activation\n",
        "        for _ in range(12):\n",
        "            x = CLIPEncoderLayer(768, 12, activation=quick_gelu)(x)\n",
        "\n",
        "        ### normalize the embedding\n",
        "        embedded = keras.layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "        super().__init__([tokens, positions], embedded, name=name)\n",
        "        \n",
        "        ### get the weights from hugging face\n",
        "        if download_weights:\n",
        "            text_encoder_weights_fpath = keras.utils.get_file(\n",
        "                origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\",\n",
        "                file_hash=\"4789e63e07c0e54d6a34a29b45ce81ece27060c499a709d556c7755b42bb0dc4\",\n",
        "            )\n",
        "            self.load_weights(text_encoder_weights_fpath)\n",
        "\n",
        "### same as above but different layers\n",
        "class TextEncoderV2(keras.Model):\n",
        "    def __init__(\n",
        "        self, max_length, vocab_size=49408, name=None, download_weights=True\n",
        "    ):\n",
        "        tokens = keras.layers.Input(\n",
        "            shape=(max_length,), dtype=\"int32\", name=\"tokens\"\n",
        "        )\n",
        "        positions = keras.layers.Input(\n",
        "            shape=(max_length,), dtype=\"int32\", name=\"positions\"\n",
        "        )\n",
        "        x = CLIPEmbedding(vocab_size, 1024, max_length)([tokens, positions])\n",
        "        for _ in range(23):\n",
        "            x = CLIPEncoderLayer(1024, 16, activation=tf.nn.gelu)(x)\n",
        "        ### normalize the embedding\n",
        "        embedded = keras.layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "        super().__init__([tokens, positions], embedded, name=name)\n",
        "\n",
        "        if download_weights:\n",
        "            text_encoder_weights_fpath = keras.utils.get_file(\n",
        "                origin=\"https://huggingface.co/ianstenbit/keras-sd2.1/resolve/main/text_encoder_v2_1.h5\",\n",
        "                file_hash=\"985002e68704e1c5c3549de332218e99c5b9b745db7171d5f31fcd9a6089f25b\",\n",
        "            )\n",
        "            self.load_weights(text_encoder_weights_fpath)\n",
        "\n",
        "### defines the activation function\n",
        "def quick_gelu(x):\n",
        "    return x * tf.sigmoid(x * 1.702)\n",
        "\n",
        "\n",
        "class CLIPEmbedding(keras.layers.Layer):\n",
        "    \"\"\" creates the embedding of the tokens and the positions and combines them \"\"\"\n",
        "    def __init__(\n",
        "        self, input_dim=49408, output_dim=768, max_length=77, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embedding = keras.layers.Embedding(input_dim, output_dim)\n",
        "        self.position_embedding = keras.layers.Embedding(max_length, output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        tokens, positions = inputs\n",
        "        ### create the embeddings\n",
        "        tokens = self.token_embedding(tokens)\n",
        "        positions = self.position_embedding(positions)\n",
        "        ### combine the two embeddings\n",
        "        return tokens + positions\n",
        "\n",
        "\n",
        "class CLIPEncoderLayer(keras.layers.Layer):\n",
        "    \"\"\" creates the layers for the encoder and sends the input through them with residual connections\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        ### layer norm to avoid overfitting\n",
        "        self.layer_norm1 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.clip_attn = CLIPAttention(embed_dim, num_heads, causal=True)\n",
        "        self.layer_norm2 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.fc1 = keras.layers.Dense(embed_dim * 4)\n",
        "        self.fc2 = keras.layers.Dense(embed_dim)\n",
        "        self.activation = activation\n",
        "\n",
        "    def call(self, inputs):\n",
        "        ### input travels through the layers with residual connections to keep the input alive\n",
        "        residual = inputs\n",
        "        x = self.layer_norm1(inputs)\n",
        "        x = self.clip_attn(x)\n",
        "        x = residual + x\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x + residual\n",
        "\n",
        "\n",
        "class CLIPAttention(keras.layers.Layer):\n",
        "    \"\"\" calculates the attention weights and embeddings \"\"\"\n",
        "    def __init__(self, embed_dim=768, num_heads=12, causal=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        ### only attending to the tokens before and not after, why?\n",
        "        self.causal = causal\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.scale = self.head_dim**-0.5\n",
        "        ### initialize embedding layers (projection) for the query, key, values and the output \n",
        "        self.q_proj = keras.layers.Dense(self.embed_dim)\n",
        "        self.k_proj = keras.layers.Dense(self.embed_dim)\n",
        "        self.v_proj = keras.layers.Dense(self.embed_dim)\n",
        "        self.out_proj = keras.layers.Dense(self.embed_dim)\n",
        "\n",
        "    def reshape_states(self, x, sequence_length, batch_size):\n",
        "        \"\"\" we need to reshape the different states for later Matrix multiplication\"\"\"\n",
        "        x = tf.reshape(\n",
        "            x, (batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        )\n",
        "        return tf.transpose(\n",
        "            x, (0, 2, 1, 3)\n",
        "        )  # bs, heads, sequence_length, head_dim\n",
        "\n",
        "    def call(self, inputs, attention_mask=None):\n",
        "        if attention_mask is None and self.causal:\n",
        "            length = tf.shape(inputs)[1]\n",
        "            attention_mask = tfnp.triu(\n",
        "                tf.ones((1, 1, length, length), dtype=self.compute_dtype)\n",
        "                * -tfnp.inf,\n",
        "                k=1,\n",
        "            )\n",
        "\n",
        "        _, tgt_len, embed_dim = inputs.shape\n",
        "        ### define the query, key and value states and reshape them\n",
        "        query_states = self.q_proj(inputs) * self.scale\n",
        "        key_states = self.reshape_states(self.k_proj(inputs), tgt_len, -1)\n",
        "        value_states = self.reshape_states(self.v_proj(inputs), tgt_len, -1)\n",
        "\n",
        "        ### projection shape depends on the target length, is used for the later reshaping\n",
        "        proj_shape = (-1, tgt_len, self.head_dim)\n",
        "        query_states = self.reshape_states(query_states, tgt_len, -1)\n",
        "        query_states = tf.reshape(query_states, proj_shape)\n",
        "        key_states = tf.reshape(key_states, proj_shape)\n",
        "\n",
        "        ### in this case the source length is equal to the target length\n",
        "        src_len = tgt_len\n",
        "        value_states = tf.reshape(value_states, proj_shape)\n",
        "\n",
        "        ###  calculate the attention weights using the query states\n",
        "        attn_weights = query_states @ tf.transpose(key_states, (0, 2, 1))\n",
        "\n",
        "        attn_weights = tf.reshape(\n",
        "            attn_weights, (-1, self.num_heads, tgt_len, src_len)\n",
        "        )\n",
        "        ### include whether the token is masked\n",
        "        attn_weights = attn_weights + attention_mask\n",
        "        attn_weights = tf.reshape(attn_weights, (-1, tgt_len, src_len))\n",
        "\n",
        "        ### apply softmax on the attention weights in order to get a probability distribution\n",
        "        attn_weights = tf.nn.softmax(attn_weights)\n",
        "        ### multiply the attention weights with the value states to determine how much attention has to be paid on which token\n",
        "        attn_output = attn_weights @ value_states\n",
        "\n",
        "        attn_output = tf.reshape(\n",
        "            attn_output, (-1, self.num_heads, tgt_len, self.head_dim)\n",
        "        )\n",
        "        attn_output = tf.transpose(attn_output, (0, 2, 1, 3))\n",
        "        attn_output = tf.reshape(attn_output, (-1, tgt_len, embed_dim))\n",
        "\n",
        "        ### return the embedding of the output\n",
        "        return self.out_proj(attn_output)"
      ]
    }
  ]
}