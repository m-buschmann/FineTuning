{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tensorflow-project/FineTuning/blob/main/two_concepts/training_two_concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow-project/FineTuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBGcW8Cv9eyo",
        "outputId": "12fbbf13-5078-4847-9946-d4a6c1e1d0c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FineTuning'...\n",
            "remote: Enumerating objects: 796, done.\u001b[K\n",
            "remote: Counting objects: 100% (432/432), done.\u001b[K\n",
            "remote: Compressing objects: 100% (233/233), done.\u001b[K\n",
            "remote: Total 796 (delta 298), reused 277 (delta 186), pack-reused 364\u001b[K\n",
            "Receiving objects: 100% (796/796), 114.77 MiB | 28.83 MiB/s, done.\n",
            "Resolving deltas: 100% (564/564), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "py_file_location = \"/content/FineTuning/two_concepts\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "py_file_location = \"/content/FineTuning/models\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "import textual_inversion_two_concepts as txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbd_Eat9rig",
        "outputId": "4914f134-d881-4330-86db-982a8f892fe3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-cv (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.9/dist-packages (from h5py) (1.22.4)\n",
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n",
            "Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n",
            "1356917/1356917 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/9zAwPyt.jpg\n",
            "8784/8784 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/qCNFRl4.jpg\n",
            "14548/14548 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/kPH9XIh.jpg\n",
            "14897/14897 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/qy1k0QK.jpg\n",
            "13911/13911 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://i.imgur.com/BLLMggR.png\n",
            "17615/17615 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/PPQ2UtM.png\n",
            "15761/15761 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/6je73G3.png\n",
            "13071/13071 [==============================] - 0s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\n",
            "492466864/492466864 [==============================] - 6s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5\n",
            "3439090152/3439090152 [==============================] - 55s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5\n",
            "198180272/198180272 [==============================] - 1s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/vae_encoder.h5\n",
            "136824240/136824240 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### EXECUTE THE FOLLOWING TWO BLOCKS ONLY IF YOU WANT TO CONTINUE TRAINING WITH SAVED WEIGTHS\n",
        "### choose where to load the weights from, either from your google drive or you load our pretrained weights\n",
        "### make sure to insert the exact name of your weight.npy\n",
        "drive.mount(\"/content/drive\")\n",
        "path = '/content/drive/MyDrive/10weights_with_two_concepts.npy'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-KgTaUHF81m",
        "outputId": "8f8663f1-25d1-48d7-f995-9b6d0b27ee8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###load the array of the weights of the text encoder from the training\n",
        "text_encoder_weights = np.load(path, allow_pickle=True)\n",
        "\n",
        "### Set the weights of the text encoder\n",
        "txt.stable_diffusion.text_encoder.set_weights(text_encoder_weights)"
      ],
      "metadata": {
        "id": "neUOdaijF-rl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### training\n",
        "txt.trainer.fit(\n",
        "    txt.train_ds,\n",
        "    epochs=5,\n",
        ")"
      ],
      "metadata": {
        "id": "XZVA19oKT2yB",
        "outputId": "308e4f79-b804-4268-84ee-d04a943d3f79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "220/220 [==============================] - 445s 2s/step - loss: 0.0614\n",
            "Epoch 2/5\n",
            "220/220 [==============================] - 349s 2s/step - loss: 0.0611\n",
            "Epoch 3/5\n",
            "220/220 [==============================] - 349s 2s/step - loss: 0.0591\n",
            "Epoch 4/5\n",
            "220/220 [==============================] - 349s 2s/step - loss: 0.0669\n",
            "Epoch 5/5\n",
            "220/220 [==============================] - 348s 2s/step - loss: 0.0622\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f80f824e130>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### choose where to save your newly generated weights\n",
        "drive.mount(\"/content/drive\")\n",
        "path = '/content/drive/MyDrive/15weights_with_two_concepts.npy'"
      ],
      "metadata": {
        "id": "CDw8MPwiHDrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5995d0e-f047-4266-daa4-14b235819fa6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###get the weights of the text encoder and save the to Google Drive\n",
        "text_encoder_weights = np.array(txt.stable_diffusion.text_encoder.get_weights())\n",
        "\n",
        "### Save the weights array to a file on your Google Drive\n",
        "np.save(path, text_encoder_weights)"
      ],
      "metadata": {
        "id": "DksSkPKsHN0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6bbe85-0b16-467c-9b5d-63e2f9614997"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0443ae8bd6c9>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  text_encoder_weights = np.array(txt.stable_diffusion.text_encoder.get_weights())\n"
          ]
        }
      ]
    }
  ]
}