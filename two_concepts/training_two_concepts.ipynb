{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tensorflow-project/FineTuning/blob/main/two_concepts/training_two_concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system('pip install -q git+https://github.com/keras-team/keras-cv.git')\n",
        "get_ipython().system('pip install -q tensorflow==2.11.0')\n",
        "get_ipython().system('pip install pyyaml h5py')"
      ],
      "metadata": {
        "id": "GG9DXY9Ogw8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_cv"
      ],
      "metadata": {
        "id": "yK7KPt7nOWWO",
        "outputId": "32c20cff-8908-4903-a472-3722f7debef7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_cv\n",
            "  Downloading keras_cv-0.4.2-py3-none-any.whl (634 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.9/634.9 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from keras_cv) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.9/dist-packages (from keras_cv) (4.8.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from keras_cv) (2022.10.31)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras_cv) (23.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (1.12.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (4.65.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (2.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (8.1.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (2.27.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (3.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (1.22.4)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (1.1.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (0.1.8)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras_cv) (5.9.4)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv) (5.12.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv) (3.15.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv) (2022.12.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from promise->tensorflow-datasets->keras_cv) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv) (1.59.0)\n",
            "Installing collected packages: keras_cv\n",
            "Successfully installed keras_cv-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow-project/FineTuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBGcW8Cv9eyo",
        "outputId": "835695df-b771-4f4f-a719-fa1a24647e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FineTuning'...\n",
            "remote: Enumerating objects: 1938, done.\u001b[K\n",
            "remote: Counting objects: 100% (901/901), done.\u001b[K\n",
            "remote: Compressing objects: 100% (293/293), done.\u001b[K\n",
            "remote: Total 1938 (delta 667), reused 821 (delta 603), pack-reused 1037\u001b[K\n",
            "Receiving objects: 100% (1938/1938), 163.51 MiB | 24.78 MiB/s, done.\n",
            "Resolving deltas: 100% (1386/1386), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import keras\n",
        "import keras_cv\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "\n",
        "### agree to mounting your Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "py_file_location = \"/content/FineTuning/two_concepts\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "py_file_location = \"/content/FineTuning/models\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "import textual_inversion_two_concepts as txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbd_Eat9rig",
        "outputId": "4071fa2b-bc91-4823-94e5-1b1298e2a65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n",
            "Mounted at /content/drive\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from h5py) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### create an instance of the Stable Diffusion model\n",
        "stable_diffusion = txt.StableDiffusion()\n",
        "\n",
        "### our new concept which is later inserted in the different prompts (for training and image generation)\n",
        "placeholder_token_broccoli = \"<my-broccoli-token>\"\n",
        "placeholder_token_emoji = \"<my-emoji-token>\"\n",
        "placeholder_token_combined = \"<my-broccoli-emoji-token>\"\n",
        "\n",
        "### Add our placeholder_tokens to our stable_diffusion Model\n",
        "stable_diffusion.tokenizer.add_tokens(placeholder_token_broccoli)\n",
        "stable_diffusion.tokenizer.add_tokens(placeholder_token_emoji)\n",
        "stable_diffusion.tokenizer.add_tokens(placeholder_token_combined)\n",
        "\n",
        "train_ds = txt.create_dataset(stable_diffusion, placeholder_token_broccoli, placeholder_token_emoji)\n",
        "\n",
        "txt.textual_preprocessing(stable_diffusion, placeholder_token_broccoli, placeholder_token_emoji, placeholder_token_combined)\n",
        "\n",
        "### beta is the diffusion rate\n",
        "noise_scheduler = txt.NoiseScheduler(\n",
        "    ### beta_start determines the amount of noise added at the start of the denoising process\n",
        "    beta_start=0.00085,\n",
        "    ### beta_end at the end of the denoising process\n",
        "    beta_end=0.012,\n",
        "    ### the beta_schedule determines that the diffusion rate increases linearly\n",
        "    beta_schedule=\"scaled_linear\",\n",
        "    train_timesteps=1000,\n",
        ")\n",
        "\n",
        "### Initialize the model we use to fine tune our concept\n",
        "trainer = txt.StableDiffusionFineTuner(stable_diffusion, noise_scheduler, name=\"trainer\")\n",
        "#t = txt.StableDiffusionFineTuner(stable_diffusion, noise_scheduler, name=\"t\")\n",
        "\n",
        "\n",
        "#EPOCHS = 50\n",
        "### learning rate decays depending on the number of epochs to avoid convergence issues in few epochs \n",
        "### in the originial tutorial a scheduler is used but we experienced to have better results without a scheduler\n",
        "\"\"\"learning_rate = keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=1e-4, decay_steps=train_ds.cardinality() * EPOCHS\n",
        ")\"\"\"\n",
        "### inizialize the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    weight_decay=0.004, learning_rate=1e-4, epsilon=1e-8, global_clipnorm=10\n",
        ")\n",
        "\n",
        "trainer.compile(\n",
        "    optimizer=optimizer,\n",
        "    # We are performing reduction manually in our train step, so none is required here.\n",
        "    loss=keras.losses.MeanSquaredError(reduction=\"none\"),\n",
        ")  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcpXhF1frApe",
        "outputId": "df828c33-758e-44ca-cf10-541f9ab61270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n",
            "Downloading data from https://i.imgur.com/9zAwPyt.jpg\n",
            "8784/8784 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/qCNFRl4.jpg\n",
            "14548/14548 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/kPH9XIh.jpg\n",
            "14897/14897 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/qy1k0QK.jpg\n",
            "13911/13911 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n",
            "1356917/1356917 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/BLLMggR.png\n",
            "17615/17615 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/PPQ2UtM.png\n",
            "15761/15761 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/6je73G3.png\n",
            "13071/13071 [==============================] - 0s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\n",
            "492466864/492466864 [==============================] - 2s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5\n",
            "3439090152/3439090152 [==============================] - 28s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5\n",
            "198180272/198180272 [==============================] - 1s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/vae_encoder.h5\n",
            "136824240/136824240 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### EXECUTE THE FOLLOWING TWO BLOCKS ONLY IF YOU WANT TO CONTINUE TRAINING WITH SAVED WEIGTHS\n",
        "### for downloading weights from Dropbox:\n",
        "### if you want to use any other service than Dropbox, change the code underneath\n",
        "\n",
        "### Replace \"\" and \"\" with the ID and name of your file\n",
        "### The file id is the string of characters between \"s/\" and the next slash \"/\"\n",
        "### example: https://www.dropbox.com/s/yttja9ihaoq2xhd/35epochs_weights_with_two_concepts.npy?dl=0\n",
        "file_id = 'yttja9ihaoq2xhd'\n",
        "\n",
        "### name óf file\n",
        "file_name = '35epochs_weights_with_two_concepts.npy'\n",
        "url = f'https://www.dropbox.com/s/{file_id}/{file_name}?dl=1'\n",
        "filename = f'{file_name}.npy'"
      ],
      "metadata": {
        "id": "V-KgTaUHF81m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "### Download the file from the URL and save it locally\n",
        "urllib.request.urlretrieve(url, filename=filename)\n",
        "\n",
        "### Load the saved numpy file using numpy.load()\n",
        "weights = np.load(filename, allow_pickle=True)\n",
        "\n",
        "### Set the weights of the text encoder\n",
        "stable_diffusion.text_encoder.set_weights(weights)"
      ],
      "metadata": {
        "id": "neUOdaijF-rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### training\n",
        "trainer.fit(\n",
        "    train_ds,\n",
        "    epochs=1,\n",
        ")"
      ],
      "metadata": {
        "id": "XZVA19oKT2yB",
        "outputId": "32e17fbe-2fc8-4d72-880f-b651abf54a28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "177/220 [=======================>......] - ETA: 1:07 - loss: 0.0275"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### choose where to save your newly generated weights\n",
        "path = '/content/drive/MyDrive/weight_with_two_concepts.npy'"
      ],
      "metadata": {
        "id": "CDw8MPwiHDrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###get the weights of the text encoder and save the to Google Drive\n",
        "text_encoder_weights = np.array(stable_diffusion.text_encoder.get_weights(), dtype=object)\n",
        "\n",
        "### Save the weights array to a file on your Google Drive\n",
        "np.save(path, text_encoder_weights)"
      ],
      "metadata": {
        "id": "DksSkPKsHN0T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}